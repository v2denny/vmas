# VMAS RLlib Benchmark

This folder contains three Python scripts to benchmark and visualize results for running VMAS environments with RLlib. These scripts are designed to evaluate the PPO algorithm on tasks like Balance and Reverse Transport.
<br>
<br>

## Contents
1. [Features](#features)
2. [How It Works](#how-it-works)
3. [Customization](#customization)
<br>
<br>

## Features
1. **`vmas_rllib.py`**:
   - Runs VMAS tasks (Balance and Reverse Transport) using the PPO algorithm with RLlib.
   - Saves videos of the training progress after every iteration.
   - Configured to utilize GPU for training (if available).

2. **`vmas_rllib_perf.py`**:
   - A performance-focused version of `vmas_rllib.py`.
   - Does not save videos or use WandB for logging.
   - Outputs training results to CSV files for graphing.

3. **`rllib_graph.py`**:
   - Reads the CSV files generated by `vmas_rllib_perf.py`.
   - Generates graphs comparing PPO algorithm performance on Balance and Reverse Transport tasks.
<br>
<br>

## How It Works

### Training Script (`vmas_rllib.py`)
1. Configures and initializes VMAS tasks (Balance and Reverse Transport).
2. Uses RLlib's PPOTrainer for training the PPO algorithm.
3. Saves videos of the training process and logs data using WandB.
4. Supports GPU utilization for faster training.

### Performance Script (`vmas_rllib_perf.py`)
1. Configured to benchmark PPO performance without additional overhead.
2. Logs training results (e.g., `episode_reward_mean`) to CSV files for each task.
3. Ideal for efficient and focused performance benchmarking.

### Plotting Script (`rllib_graph.py`)
1. Reads the CSV files generated by `vmas_rllib_perf.py`.
2. Plots graphs comparing PPO performance on Balance and Reverse Transport tasks.
3. Outputs a single PNG file showing the comparison.
<br>
<br>

## Customization

- **Scenario Configuration**:
  Modify the `scenario_name` variable in `vmas_rllib.py` or `vmas_rllib_perf.py` to switch between Balance and Reverse Transport.

- **Number of Agents**:
  Change the `n_agents` variable to adjust the number of agents in the environment.

- **GPU/CPU Execution**:
  Update the `vmas_device` variable to `"cuda"` for GPU or `"cpu"` for CPU.

- **Number of Training Iterations**:
  Adjust the `training_iteration` parameter in the `tune.run` method in both training scripts.

- **Environment Parameters**:
  Modify `max_steps`, `num_vectorized_envs`, and `num_workers` variables to configure the maximum steps per episode, the number of vectorized environments, and the number of workers, respectively.

- **Checkpoints**:
  Set `checkpoint_path` in `vmas_rllib.py` or `vmas_rllib_perf.py` to load a specific checkpoint. If using video saving, update `self.iteration_counter` in `VideoSavingCallbacks` to ensure videos are saved in the correct order.

- **Log Directory**:
  Change `local_dir` in the `tune.run` method to specify a different logging directory.

- **Batch Size**:
  Update the `"train_batch_size"` parameter in the `tune.run` configuration to adjust the training batch size.